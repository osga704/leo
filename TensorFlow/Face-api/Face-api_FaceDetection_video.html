<!-- 
Author : ChungYi Fu (Kaohsiung, Taiwan)   2020/1/4 22:00
https://www.facebook.com/francefu

Try it!
https://fustyles.github.io/webduino/TensorFlow/Face-api/Face-api_FaceDetection_video.html

How to enable WebGL in Chrome.
https://superuser.com/questions/836832/how-can-i-enable-webgl-in-my-browser
-->
<!DOCTYPE html>
<head>
  <title>Face Detection</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src='face-api.min.js'></script>
<style>
  #webcam-container {
    padding: 0;
  }
  #hide {
    position: absolute;
    top: 250px;
    left: 0;
  }
  #message {
    position: absolute;
    top: 280px;
    left: 0;
    color:red;
  }
  canvas {
    position: absolute;
    top: 0;
    left: 0;
  }
  video {
    position: absolute;
    top: 0;
    left: 0;
  }
</style>
</head>
<body>
<video id="webcam" autoplay muted playsinline></video>
<div id="webcam-container"></div>
<div id="hide">
<input type="checkbox" onclick="if (this.checked) camera.style.display='none'; else camera.style.display='block';" value="">Hide Video
</div>
<div id="message">Please wait for loading model.</div>
  
<script>
const camera = document.getElementById('webcam');
const modelPath = './';
let currentStream;
let displaySize = { width:320, height: 240 }
let canvas;
let faceDetection;
let message = document.getElementById('message'); 
  
  ObjectDetect();  
  
  function ObjectDetect() {
    Promise.all([
      faceapi.nets.tinyFaceDetector.load(modelPath),
      faceapi.nets.faceLandmark68TinyNet.load(modelPath),
      faceapi.nets.faceRecognitionNet.load(modelPath),
      faceapi.nets.faceExpressionNet.load(modelPath),
      faceapi.nets.ageGenderNet.load(modelPath)
    ]).then(function(){
      startvideo();
    })
  }  
  
  function startvideo() {
    navigator.mediaDevices
      .getUserMedia({
        audio: false,
        video: {
          facingMode: "user",
          width: 320,
          height: 240
        }
      })
      .then(stream => {
        camera.srcObject = stream
        camera.onloadedmetadata = () => {       
          camera.play();
          if( document.getElementsByTagName("canvas").length == 0 )
          {
            canvas = faceapi.createCanvasFromMedia(camera)
            document.getElementById('webcam-container').append(canvas)
            faceapi.matchDimensions(canvas, displaySize)
          }  
          //camera.style.visibility="hidden";     
          DetectVideo();
        }
      })   
  }
                        
  async function DetectVideo() {
      const detections = await faceapi.detectAllFaces(camera, new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks(true).withFaceExpressions().withAgeAndGender()
      const resizedDetections = faceapi.resizeResults(detections, displaySize)
      canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height)
      faceapi.draw.drawDetections(canvas, resizedDetections)
      faceapi.draw.drawFaceLandmarks(canvas, resizedDetections)
      faceapi.draw.drawFaceExpressions(canvas, resizedDetections)
      resizedDetections.forEach(result => {
        const { age, gender, genderProbability } = result
        message.innerHTML = JSON.stringify(result);
        new faceapi.draw.DrawTextField(
          [
            `${faceapi.round(age, 0)} years`,
            `${gender} (${faceapi.round(genderProbability)})`
          ],
          result.detection.box.bottomRight
        ).draw(canvas)
      })
      try { 
        document.createEvent("TouchEvent");
        setTimeout(function(){DetectVideo();},250);
      }
      catch(e) { 
        setTimeout(function(){DetectVideo();},150);
      } 
  }
</script>

</body>
</html>
